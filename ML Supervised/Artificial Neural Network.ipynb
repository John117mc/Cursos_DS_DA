{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262702bd",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais\n",
    "\n",
    "* Reconhecimento Facial, Processamento de Linguagem Natural\n",
    "* Muitos dados (Big Data) e problemas complexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe6816b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:30:12.359599Z",
     "start_time": "2021-09-25T18:29:37.792105Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import scikitplot as skplt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf62bf",
   "metadata": {},
   "source": [
    "## Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324d6084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:30:12.656265Z",
     "start_time": "2021-09-25T18:30:12.392653Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/credit.pkl', 'rb') as f:\n",
    "    x_credit_train, y_credit_train, x_credit_test, y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4872d2cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:57:04.519748Z",
     "start_time": "2021-09-25T18:57:04.507747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Camada oculta (Entrada + Sa√≠da) / 2\n",
    "(3 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9293db7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:31:38.475736Z",
     "start_time": "2021-09-25T18:31:32.844466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.85130219\n",
      "Iteration 2, loss = 0.83868480\n",
      "Iteration 3, loss = 0.82682574\n",
      "Iteration 4, loss = 0.81568385\n",
      "Iteration 5, loss = 0.80504118\n",
      "Iteration 6, loss = 0.79507482\n",
      "Iteration 7, loss = 0.78568309\n",
      "Iteration 8, loss = 0.77694458\n",
      "Iteration 9, loss = 0.76859254\n",
      "Iteration 10, loss = 0.76066644\n",
      "Iteration 11, loss = 0.75312650\n",
      "Iteration 12, loss = 0.74596132\n",
      "Iteration 13, loss = 0.73913101\n",
      "Iteration 14, loss = 0.73264111\n",
      "Iteration 15, loss = 0.72633125\n",
      "Iteration 16, loss = 0.72046581\n",
      "Iteration 17, loss = 0.71479965\n",
      "Iteration 18, loss = 0.70932143\n",
      "Iteration 19, loss = 0.70406670\n",
      "Iteration 20, loss = 0.69908412\n",
      "Iteration 21, loss = 0.69427356\n",
      "Iteration 22, loss = 0.68952459\n",
      "Iteration 23, loss = 0.68512302\n",
      "Iteration 24, loss = 0.68073053\n",
      "Iteration 25, loss = 0.67660305\n",
      "Iteration 26, loss = 0.67259084\n",
      "Iteration 27, loss = 0.66868791\n",
      "Iteration 28, loss = 0.66493039\n",
      "Iteration 29, loss = 0.66144482\n",
      "Iteration 30, loss = 0.65802379\n",
      "Iteration 31, loss = 0.65471713\n",
      "Iteration 32, loss = 0.65159515\n",
      "Iteration 33, loss = 0.64867982\n",
      "Iteration 34, loss = 0.64574822\n",
      "Iteration 35, loss = 0.64310089\n",
      "Iteration 36, loss = 0.64045603\n",
      "Iteration 37, loss = 0.63799591\n",
      "Iteration 38, loss = 0.63561648\n",
      "Iteration 39, loss = 0.63326994\n",
      "Iteration 40, loss = 0.63107818\n",
      "Iteration 41, loss = 0.62890957\n",
      "Iteration 42, loss = 0.62678027\n",
      "Iteration 43, loss = 0.62468395\n",
      "Iteration 44, loss = 0.62259185\n",
      "Iteration 45, loss = 0.62055881\n",
      "Iteration 46, loss = 0.61852110\n",
      "Iteration 47, loss = 0.61651715\n",
      "Iteration 48, loss = 0.61449601\n",
      "Iteration 49, loss = 0.61253154\n",
      "Iteration 50, loss = 0.61058341\n",
      "Iteration 51, loss = 0.60861269\n",
      "Iteration 52, loss = 0.60665614\n",
      "Iteration 53, loss = 0.60474287\n",
      "Iteration 54, loss = 0.60282134\n",
      "Iteration 55, loss = 0.60094772\n",
      "Iteration 56, loss = 0.59900622\n",
      "Iteration 57, loss = 0.59716603\n",
      "Iteration 58, loss = 0.59528749\n",
      "Iteration 59, loss = 0.59343705\n",
      "Iteration 60, loss = 0.59159158\n",
      "Iteration 61, loss = 0.58976833\n",
      "Iteration 62, loss = 0.58794046\n",
      "Iteration 63, loss = 0.58613836\n",
      "Iteration 64, loss = 0.58434911\n",
      "Iteration 65, loss = 0.58260061\n",
      "Iteration 66, loss = 0.58085030\n",
      "Iteration 67, loss = 0.57909958\n",
      "Iteration 68, loss = 0.57738950\n",
      "Iteration 69, loss = 0.57565811\n",
      "Iteration 70, loss = 0.57394357\n",
      "Iteration 71, loss = 0.57228845\n",
      "Iteration 72, loss = 0.57059636\n",
      "Iteration 73, loss = 0.56894977\n",
      "Iteration 74, loss = 0.56728910\n",
      "Iteration 75, loss = 0.56566302\n",
      "Iteration 76, loss = 0.56404474\n",
      "Iteration 77, loss = 0.56242981\n",
      "Iteration 78, loss = 0.56084343\n",
      "Iteration 79, loss = 0.55924611\n",
      "Iteration 80, loss = 0.55770803\n",
      "Iteration 81, loss = 0.55612410\n",
      "Iteration 82, loss = 0.55456440\n",
      "Iteration 83, loss = 0.55301603\n",
      "Iteration 84, loss = 0.55150674\n",
      "Iteration 85, loss = 0.54996874\n",
      "Iteration 86, loss = 0.54844150\n",
      "Iteration 87, loss = 0.54694530\n",
      "Iteration 88, loss = 0.54544110\n",
      "Iteration 89, loss = 0.54396916\n",
      "Iteration 90, loss = 0.54250730\n",
      "Iteration 91, loss = 0.54103504\n",
      "Iteration 92, loss = 0.53957457\n",
      "Iteration 93, loss = 0.53814455\n",
      "Iteration 94, loss = 0.53670229\n",
      "Iteration 95, loss = 0.53524492\n",
      "Iteration 96, loss = 0.53369377\n",
      "Iteration 97, loss = 0.53178867\n",
      "Iteration 98, loss = 0.52844721\n",
      "Iteration 99, loss = 0.52304766\n",
      "Iteration 100, loss = 0.51507159\n",
      "Iteration 101, loss = 0.50706905\n",
      "Iteration 102, loss = 0.50157916\n",
      "Iteration 103, loss = 0.49673525\n",
      "Iteration 104, loss = 0.49326780\n",
      "Iteration 105, loss = 0.48987713\n",
      "Iteration 106, loss = 0.48664563\n",
      "Iteration 107, loss = 0.48351802\n",
      "Iteration 108, loss = 0.48051616\n",
      "Iteration 109, loss = 0.47764124\n",
      "Iteration 110, loss = 0.47475968\n",
      "Iteration 111, loss = 0.47194869\n",
      "Iteration 112, loss = 0.46927945\n",
      "Iteration 113, loss = 0.46653713\n",
      "Iteration 114, loss = 0.46388153\n",
      "Iteration 115, loss = 0.46127442\n",
      "Iteration 116, loss = 0.45874124\n",
      "Iteration 117, loss = 0.45620795\n",
      "Iteration 118, loss = 0.45367831\n",
      "Iteration 119, loss = 0.45124883\n",
      "Iteration 120, loss = 0.44884356\n",
      "Iteration 121, loss = 0.44647845\n",
      "Iteration 122, loss = 0.44418557\n",
      "Iteration 123, loss = 0.44185405\n",
      "Iteration 124, loss = 0.43952159\n",
      "Iteration 125, loss = 0.43723002\n",
      "Iteration 126, loss = 0.43496820\n",
      "Iteration 127, loss = 0.43276858\n",
      "Iteration 128, loss = 0.43056515\n",
      "Iteration 129, loss = 0.42834531\n",
      "Iteration 130, loss = 0.42615623\n",
      "Iteration 131, loss = 0.42400333\n",
      "Iteration 132, loss = 0.42187503\n",
      "Iteration 133, loss = 0.41972299\n",
      "Iteration 134, loss = 0.41763412\n",
      "Iteration 135, loss = 0.41557681\n",
      "Iteration 136, loss = 0.41356490\n",
      "Iteration 137, loss = 0.41147619\n",
      "Iteration 138, loss = 0.40949979\n",
      "Iteration 139, loss = 0.40753501\n",
      "Iteration 140, loss = 0.40554633\n",
      "Iteration 141, loss = 0.40362948\n",
      "Iteration 142, loss = 0.40167395\n",
      "Iteration 143, loss = 0.39978206\n",
      "Iteration 144, loss = 0.39794972\n",
      "Iteration 145, loss = 0.39604298\n",
      "Iteration 146, loss = 0.39422809\n",
      "Iteration 147, loss = 0.39237955\n",
      "Iteration 148, loss = 0.39056617\n",
      "Iteration 149, loss = 0.38884227\n",
      "Iteration 150, loss = 0.38699037\n",
      "Iteration 151, loss = 0.38525271\n",
      "Iteration 152, loss = 0.38361829\n",
      "Iteration 153, loss = 0.38179399\n",
      "Iteration 154, loss = 0.38011152\n",
      "Iteration 155, loss = 0.37839498\n",
      "Iteration 156, loss = 0.37672087\n",
      "Iteration 157, loss = 0.37504234\n",
      "Iteration 158, loss = 0.37338857\n",
      "Iteration 159, loss = 0.37185398\n",
      "Iteration 160, loss = 0.37016184\n",
      "Iteration 161, loss = 0.36857469\n",
      "Iteration 162, loss = 0.36699358\n",
      "Iteration 163, loss = 0.36544480\n",
      "Iteration 164, loss = 0.36389371\n",
      "Iteration 165, loss = 0.36237081\n",
      "Iteration 166, loss = 0.36080266\n",
      "Iteration 167, loss = 0.35924098\n",
      "Iteration 168, loss = 0.35773813\n",
      "Iteration 169, loss = 0.35625217\n",
      "Iteration 170, loss = 0.35481152\n",
      "Iteration 171, loss = 0.35329715\n",
      "Iteration 172, loss = 0.35180691\n",
      "Iteration 173, loss = 0.35036480\n",
      "Iteration 174, loss = 0.34894427\n",
      "Iteration 175, loss = 0.34752950\n",
      "Iteration 176, loss = 0.34611001\n",
      "Iteration 177, loss = 0.34467121\n",
      "Iteration 178, loss = 0.34328983\n",
      "Iteration 179, loss = 0.34192200\n",
      "Iteration 180, loss = 0.34057845\n",
      "Iteration 181, loss = 0.33919127\n",
      "Iteration 182, loss = 0.33784148\n",
      "Iteration 183, loss = 0.33651206\n",
      "Iteration 184, loss = 0.33519141\n",
      "Iteration 185, loss = 0.33387855\n",
      "Iteration 186, loss = 0.33255979\n",
      "Iteration 187, loss = 0.33128652\n",
      "Iteration 188, loss = 0.33003273\n",
      "Iteration 189, loss = 0.32873286\n",
      "Iteration 190, loss = 0.32748475\n",
      "Iteration 191, loss = 0.32622517\n",
      "Iteration 192, loss = 0.32505681\n",
      "Iteration 193, loss = 0.32372838\n",
      "Iteration 194, loss = 0.32251313\n",
      "Iteration 195, loss = 0.32129728\n",
      "Iteration 196, loss = 0.32011859\n",
      "Iteration 197, loss = 0.31891412\n",
      "Iteration 198, loss = 0.31773183\n",
      "Iteration 199, loss = 0.31655750\n",
      "Iteration 200, loss = 0.31540576\n",
      "Iteration 201, loss = 0.31424751\n",
      "Iteration 202, loss = 0.31312039\n",
      "Iteration 203, loss = 0.31196250\n",
      "Iteration 204, loss = 0.31088263\n",
      "Iteration 205, loss = 0.30974843\n",
      "Iteration 206, loss = 0.30862360\n",
      "Iteration 207, loss = 0.30749315\n",
      "Iteration 208, loss = 0.30638789\n",
      "Iteration 209, loss = 0.30530472\n",
      "Iteration 210, loss = 0.30424094\n",
      "Iteration 211, loss = 0.30315260\n",
      "Iteration 212, loss = 0.30212220\n",
      "Iteration 213, loss = 0.30107887\n",
      "Iteration 214, loss = 0.30001043\n",
      "Iteration 215, loss = 0.29899866\n",
      "Iteration 216, loss = 0.29792050\n",
      "Iteration 217, loss = 0.29689877\n",
      "Iteration 218, loss = 0.29589436\n",
      "Iteration 219, loss = 0.29491365\n",
      "Iteration 220, loss = 0.29389156\n",
      "Iteration 221, loss = 0.29284821\n",
      "Iteration 222, loss = 0.29188899\n",
      "Iteration 223, loss = 0.29094842\n",
      "Iteration 224, loss = 0.28994977\n",
      "Iteration 225, loss = 0.28896500\n",
      "Iteration 226, loss = 0.28799583\n",
      "Iteration 227, loss = 0.28709655\n",
      "Iteration 228, loss = 0.28611698\n",
      "Iteration 229, loss = 0.28518380\n",
      "Iteration 230, loss = 0.28424056\n",
      "Iteration 231, loss = 0.28332944\n",
      "Iteration 232, loss = 0.28239658\n",
      "Iteration 233, loss = 0.28147194\n",
      "Iteration 234, loss = 0.28060993\n",
      "Iteration 235, loss = 0.27968142\n",
      "Iteration 236, loss = 0.27878600\n",
      "Iteration 237, loss = 0.27791929\n",
      "Iteration 238, loss = 0.27704949\n",
      "Iteration 239, loss = 0.27616596\n",
      "Iteration 240, loss = 0.27532440\n",
      "Iteration 241, loss = 0.27446407\n",
      "Iteration 242, loss = 0.27359523\n",
      "Iteration 243, loss = 0.27271592\n",
      "Iteration 244, loss = 0.27187901\n",
      "Iteration 245, loss = 0.27105186\n",
      "Iteration 246, loss = 0.27021182\n",
      "Iteration 247, loss = 0.26946582\n",
      "Iteration 248, loss = 0.26855923\n",
      "Iteration 249, loss = 0.26776832\n",
      "Iteration 250, loss = 0.26695546\n",
      "Iteration 251, loss = 0.26612709\n",
      "Iteration 252, loss = 0.26532893\n",
      "Iteration 253, loss = 0.26457963\n",
      "Iteration 254, loss = 0.26375261\n",
      "Iteration 255, loss = 0.26296876\n",
      "Iteration 256, loss = 0.26219338\n",
      "Iteration 257, loss = 0.26140075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.26067368\n",
      "Iteration 259, loss = 0.25989375\n",
      "Iteration 260, loss = 0.25911846\n",
      "Iteration 261, loss = 0.25836878\n",
      "Iteration 262, loss = 0.25764501\n",
      "Iteration 263, loss = 0.25688063\n",
      "Iteration 264, loss = 0.25628530\n",
      "Iteration 265, loss = 0.25539825\n",
      "Iteration 266, loss = 0.25464801\n",
      "Iteration 267, loss = 0.25396704\n",
      "Iteration 268, loss = 0.25327563\n",
      "Iteration 269, loss = 0.25262387\n",
      "Iteration 270, loss = 0.25183909\n",
      "Iteration 271, loss = 0.25111804\n",
      "Iteration 272, loss = 0.25042262\n",
      "Iteration 273, loss = 0.24972963\n",
      "Iteration 274, loss = 0.24903809\n",
      "Iteration 275, loss = 0.24835797\n",
      "Iteration 276, loss = 0.24766695\n",
      "Iteration 277, loss = 0.24703888\n",
      "Iteration 278, loss = 0.24635741\n",
      "Iteration 279, loss = 0.24570385\n",
      "Iteration 280, loss = 0.24507050\n",
      "Iteration 281, loss = 0.24438014\n",
      "Iteration 282, loss = 0.24368004\n",
      "Iteration 283, loss = 0.24302987\n",
      "Iteration 284, loss = 0.24241676\n",
      "Iteration 285, loss = 0.24175097\n",
      "Iteration 286, loss = 0.24112769\n",
      "Iteration 287, loss = 0.24049567\n",
      "Iteration 288, loss = 0.23983610\n",
      "Iteration 289, loss = 0.23925871\n",
      "Iteration 290, loss = 0.23863792\n",
      "Iteration 291, loss = 0.23796220\n",
      "Iteration 292, loss = 0.23735100\n",
      "Iteration 293, loss = 0.23679358\n",
      "Iteration 294, loss = 0.23620239\n",
      "Iteration 295, loss = 0.23559466\n",
      "Iteration 296, loss = 0.23494910\n",
      "Iteration 297, loss = 0.23436912\n",
      "Iteration 298, loss = 0.23375376\n",
      "Iteration 299, loss = 0.23324262\n",
      "Iteration 300, loss = 0.23268281\n",
      "Iteration 301, loss = 0.23201477\n",
      "Iteration 302, loss = 0.23146762\n",
      "Iteration 303, loss = 0.23086920\n",
      "Iteration 304, loss = 0.23032751\n",
      "Iteration 305, loss = 0.22974421\n",
      "Iteration 306, loss = 0.22918812\n",
      "Iteration 307, loss = 0.22860505\n",
      "Iteration 308, loss = 0.22814798\n",
      "Iteration 309, loss = 0.22755084\n",
      "Iteration 310, loss = 0.22708125\n",
      "Iteration 311, loss = 0.22644842\n",
      "Iteration 312, loss = 0.22589117\n",
      "Iteration 313, loss = 0.22535671\n",
      "Iteration 314, loss = 0.22484730\n",
      "Iteration 315, loss = 0.22429883\n",
      "Iteration 316, loss = 0.22376013\n",
      "Iteration 317, loss = 0.22326336\n",
      "Iteration 318, loss = 0.22270458\n",
      "Iteration 319, loss = 0.22220713\n",
      "Iteration 320, loss = 0.22169318\n",
      "Iteration 321, loss = 0.22115383\n",
      "Iteration 322, loss = 0.22064986\n",
      "Iteration 323, loss = 0.22015089\n",
      "Iteration 324, loss = 0.21970660\n",
      "Iteration 325, loss = 0.21916615\n",
      "Iteration 326, loss = 0.21865577\n",
      "Iteration 327, loss = 0.21814619\n",
      "Iteration 328, loss = 0.21766622\n",
      "Iteration 329, loss = 0.21716366\n",
      "Iteration 330, loss = 0.21673516\n",
      "Iteration 331, loss = 0.21620030\n",
      "Iteration 332, loss = 0.21571786\n",
      "Iteration 333, loss = 0.21527394\n",
      "Iteration 334, loss = 0.21481423\n",
      "Iteration 335, loss = 0.21426913\n",
      "Iteration 336, loss = 0.21382723\n",
      "Iteration 337, loss = 0.21335208\n",
      "Iteration 338, loss = 0.21288704\n",
      "Iteration 339, loss = 0.21242169\n",
      "Iteration 340, loss = 0.21197844\n",
      "Iteration 341, loss = 0.21150135\n",
      "Iteration 342, loss = 0.21104387\n",
      "Iteration 343, loss = 0.21064967\n",
      "Iteration 344, loss = 0.21013998\n",
      "Iteration 345, loss = 0.20968881\n",
      "Iteration 346, loss = 0.20925189\n",
      "Iteration 347, loss = 0.20883378\n",
      "Iteration 348, loss = 0.20844699\n",
      "Iteration 349, loss = 0.20789261\n",
      "Iteration 350, loss = 0.20747197\n",
      "Iteration 351, loss = 0.20700926\n",
      "Iteration 352, loss = 0.20657414\n",
      "Iteration 353, loss = 0.20612489\n",
      "Iteration 354, loss = 0.20569207\n",
      "Iteration 355, loss = 0.20525434\n",
      "Iteration 356, loss = 0.20487018\n",
      "Iteration 357, loss = 0.20443382\n",
      "Iteration 358, loss = 0.20395013\n",
      "Iteration 359, loss = 0.20347013\n",
      "Iteration 360, loss = 0.20296598\n",
      "Iteration 361, loss = 0.20245932\n",
      "Iteration 362, loss = 0.20187951\n",
      "Iteration 363, loss = 0.20122294\n",
      "Iteration 364, loss = 0.20025887\n",
      "Iteration 365, loss = 0.19914789\n",
      "Iteration 366, loss = 0.19813724\n",
      "Iteration 367, loss = 0.19705417\n",
      "Iteration 368, loss = 0.19575067\n",
      "Iteration 369, loss = 0.19463526\n",
      "Iteration 370, loss = 0.19294237\n",
      "Iteration 371, loss = 0.19110530\n",
      "Iteration 372, loss = 0.18839599\n",
      "Iteration 373, loss = 0.18562316\n",
      "Iteration 374, loss = 0.18314963\n",
      "Iteration 375, loss = 0.18103855\n",
      "Iteration 376, loss = 0.17921755\n",
      "Iteration 377, loss = 0.17779611\n",
      "Iteration 378, loss = 0.17635452\n",
      "Iteration 379, loss = 0.17485621\n",
      "Iteration 380, loss = 0.17346787\n",
      "Iteration 381, loss = 0.17221043\n",
      "Iteration 382, loss = 0.17108313\n",
      "Iteration 383, loss = 0.16999989\n",
      "Iteration 384, loss = 0.16898547\n",
      "Iteration 385, loss = 0.16792356\n",
      "Iteration 386, loss = 0.16687691\n",
      "Iteration 387, loss = 0.16593480\n",
      "Iteration 388, loss = 0.16496825\n",
      "Iteration 389, loss = 0.16401681\n",
      "Iteration 390, loss = 0.16310477\n",
      "Iteration 391, loss = 0.16220943\n",
      "Iteration 392, loss = 0.16130851\n",
      "Iteration 393, loss = 0.16043541\n",
      "Iteration 394, loss = 0.15963505\n",
      "Iteration 395, loss = 0.15882910\n",
      "Iteration 396, loss = 0.15795371\n",
      "Iteration 397, loss = 0.15719629\n",
      "Iteration 398, loss = 0.15641918\n",
      "Iteration 399, loss = 0.15552088\n",
      "Iteration 400, loss = 0.15474632\n",
      "Iteration 401, loss = 0.15396907\n",
      "Iteration 402, loss = 0.15310893\n",
      "Iteration 403, loss = 0.15232590\n",
      "Iteration 404, loss = 0.15158022\n",
      "Iteration 405, loss = 0.15080940\n",
      "Iteration 406, loss = 0.15005821\n",
      "Iteration 407, loss = 0.14937854\n",
      "Iteration 408, loss = 0.14863090\n",
      "Iteration 409, loss = 0.14784989\n",
      "Iteration 410, loss = 0.14716077\n",
      "Iteration 411, loss = 0.14642369\n",
      "Iteration 412, loss = 0.14569811\n",
      "Iteration 413, loss = 0.14495673\n",
      "Iteration 414, loss = 0.14427209\n",
      "Iteration 415, loss = 0.14358143\n",
      "Iteration 416, loss = 0.14290320\n",
      "Iteration 417, loss = 0.14222567\n",
      "Iteration 418, loss = 0.14153441\n",
      "Iteration 419, loss = 0.14084627\n",
      "Iteration 420, loss = 0.14021995\n",
      "Iteration 421, loss = 0.13955793\n",
      "Iteration 422, loss = 0.13887744\n",
      "Iteration 423, loss = 0.13821231\n",
      "Iteration 424, loss = 0.13755686\n",
      "Iteration 425, loss = 0.13691487\n",
      "Iteration 426, loss = 0.13631933\n",
      "Iteration 427, loss = 0.13565725\n",
      "Iteration 428, loss = 0.13503773\n",
      "Iteration 429, loss = 0.13438705\n",
      "Iteration 430, loss = 0.13387889\n",
      "Iteration 431, loss = 0.13317111\n",
      "Iteration 432, loss = 0.13253991\n",
      "Iteration 433, loss = 0.13194631\n",
      "Iteration 434, loss = 0.13124708\n",
      "Iteration 435, loss = 0.13062953\n",
      "Iteration 436, loss = 0.12999646\n",
      "Iteration 437, loss = 0.12940088\n",
      "Iteration 438, loss = 0.12874834\n",
      "Iteration 439, loss = 0.12815976\n",
      "Iteration 440, loss = 0.12756269\n",
      "Iteration 441, loss = 0.12694272\n",
      "Iteration 442, loss = 0.12634084\n",
      "Iteration 443, loss = 0.12580682\n",
      "Iteration 444, loss = 0.12519463\n",
      "Iteration 445, loss = 0.12459244\n",
      "Iteration 446, loss = 0.12403765\n",
      "Iteration 447, loss = 0.12348560\n",
      "Iteration 448, loss = 0.12288469\n",
      "Iteration 449, loss = 0.12237523\n",
      "Iteration 450, loss = 0.12174519\n",
      "Iteration 451, loss = 0.12122531\n",
      "Iteration 452, loss = 0.12063408\n",
      "Iteration 453, loss = 0.12008560\n",
      "Iteration 454, loss = 0.11956167\n",
      "Iteration 455, loss = 0.11898456\n",
      "Iteration 456, loss = 0.11841299\n",
      "Iteration 457, loss = 0.11790234\n",
      "Iteration 458, loss = 0.11739549\n",
      "Iteration 459, loss = 0.11685393\n",
      "Iteration 460, loss = 0.11632672\n",
      "Iteration 461, loss = 0.11575857\n",
      "Iteration 462, loss = 0.11525740\n",
      "Iteration 463, loss = 0.11479656\n",
      "Iteration 464, loss = 0.11423048\n",
      "Iteration 465, loss = 0.11372359\n",
      "Iteration 466, loss = 0.11321496\n",
      "Iteration 467, loss = 0.11271169\n",
      "Iteration 468, loss = 0.11220609\n",
      "Iteration 469, loss = 0.11170907\n",
      "Iteration 470, loss = 0.11120882\n",
      "Iteration 471, loss = 0.11072574\n",
      "Iteration 472, loss = 0.11025897\n",
      "Iteration 473, loss = 0.10973982\n",
      "Iteration 474, loss = 0.10926301\n",
      "Iteration 475, loss = 0.10884376\n",
      "Iteration 476, loss = 0.10828303\n",
      "Iteration 477, loss = 0.10782082\n",
      "Iteration 478, loss = 0.10737910\n",
      "Iteration 479, loss = 0.10688904\n",
      "Iteration 480, loss = 0.10653462\n",
      "Iteration 481, loss = 0.10593012\n",
      "Iteration 482, loss = 0.10556951\n",
      "Iteration 483, loss = 0.10505334\n",
      "Iteration 484, loss = 0.10457957\n",
      "Iteration 485, loss = 0.10411760\n",
      "Iteration 486, loss = 0.10367359\n",
      "Iteration 487, loss = 0.10320456\n",
      "Iteration 488, loss = 0.10285969\n",
      "Iteration 489, loss = 0.10231363\n",
      "Iteration 490, loss = 0.10189406\n",
      "Iteration 491, loss = 0.10145833\n",
      "Iteration 492, loss = 0.10101060\n",
      "Iteration 493, loss = 0.10056925\n",
      "Iteration 494, loss = 0.10014098\n",
      "Iteration 495, loss = 0.09971580\n",
      "Iteration 496, loss = 0.09935874\n",
      "Iteration 497, loss = 0.09886323\n",
      "Iteration 498, loss = 0.09850166\n",
      "Iteration 499, loss = 0.09800929\n",
      "Iteration 500, loss = 0.09759499\n",
      "Iteration 501, loss = 0.09720068\n",
      "Iteration 502, loss = 0.09678623\n",
      "Iteration 503, loss = 0.09634918\n",
      "Iteration 504, loss = 0.09598176\n",
      "Iteration 505, loss = 0.09553120\n",
      "Iteration 506, loss = 0.09514576\n",
      "Iteration 507, loss = 0.09471574\n",
      "Iteration 508, loss = 0.09430838\n",
      "Iteration 509, loss = 0.09391613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 510, loss = 0.09349972\n",
      "Iteration 511, loss = 0.09310649\n",
      "Iteration 512, loss = 0.09270086\n",
      "Iteration 513, loss = 0.09232600\n",
      "Iteration 514, loss = 0.09193155\n",
      "Iteration 515, loss = 0.09152037\n",
      "Iteration 516, loss = 0.09115393\n",
      "Iteration 517, loss = 0.09078435\n",
      "Iteration 518, loss = 0.09034697\n",
      "Iteration 519, loss = 0.09011211\n",
      "Iteration 520, loss = 0.08963779\n",
      "Iteration 521, loss = 0.08927438\n",
      "Iteration 522, loss = 0.08889247\n",
      "Iteration 523, loss = 0.08854488\n",
      "Iteration 524, loss = 0.08814724\n",
      "Iteration 525, loss = 0.08778454\n",
      "Iteration 526, loss = 0.08740196\n",
      "Iteration 527, loss = 0.08704302\n",
      "Iteration 528, loss = 0.08665397\n",
      "Iteration 529, loss = 0.08630449\n",
      "Iteration 530, loss = 0.08594917\n",
      "Iteration 531, loss = 0.08559592\n",
      "Iteration 532, loss = 0.08523717\n",
      "Iteration 533, loss = 0.08489410\n",
      "Iteration 534, loss = 0.08454278\n",
      "Iteration 535, loss = 0.08418741\n",
      "Iteration 536, loss = 0.08384884\n",
      "Iteration 537, loss = 0.08350181\n",
      "Iteration 538, loss = 0.08315049\n",
      "Iteration 539, loss = 0.08281008\n",
      "Iteration 540, loss = 0.08247215\n",
      "Iteration 541, loss = 0.08211432\n",
      "Iteration 542, loss = 0.08183805\n",
      "Iteration 543, loss = 0.08149171\n",
      "Iteration 544, loss = 0.08117615\n",
      "Iteration 545, loss = 0.08083300\n",
      "Iteration 546, loss = 0.08048472\n",
      "Iteration 547, loss = 0.08017009\n",
      "Iteration 548, loss = 0.07985578\n",
      "Iteration 549, loss = 0.07950186\n",
      "Iteration 550, loss = 0.07918260\n",
      "Iteration 551, loss = 0.07885222\n",
      "Iteration 552, loss = 0.07854243\n",
      "Iteration 553, loss = 0.07826291\n",
      "Iteration 554, loss = 0.07794266\n",
      "Iteration 555, loss = 0.07762651\n",
      "Iteration 556, loss = 0.07736146\n",
      "Iteration 557, loss = 0.07700549\n",
      "Iteration 558, loss = 0.07671248\n",
      "Iteration 559, loss = 0.07638822\n",
      "Iteration 560, loss = 0.07613963\n",
      "Iteration 561, loss = 0.07585633\n",
      "Iteration 562, loss = 0.07553438\n",
      "Iteration 563, loss = 0.07531251\n",
      "Iteration 564, loss = 0.07492140\n",
      "Iteration 565, loss = 0.07463473\n",
      "Iteration 566, loss = 0.07433695\n",
      "Iteration 567, loss = 0.07406802\n",
      "Iteration 568, loss = 0.07379934\n",
      "Iteration 569, loss = 0.07348972\n",
      "Iteration 570, loss = 0.07316701\n",
      "Iteration 571, loss = 0.07296317\n",
      "Iteration 572, loss = 0.07270877\n",
      "Iteration 573, loss = 0.07236840\n",
      "Iteration 574, loss = 0.07210729\n",
      "Iteration 575, loss = 0.07179875\n",
      "Iteration 576, loss = 0.07152438\n",
      "Iteration 577, loss = 0.07123981\n",
      "Iteration 578, loss = 0.07101243\n",
      "Iteration 579, loss = 0.07077380\n",
      "Iteration 580, loss = 0.07045863\n",
      "Iteration 581, loss = 0.07023402\n",
      "Iteration 582, loss = 0.07004533\n",
      "Iteration 583, loss = 0.06970455\n",
      "Iteration 584, loss = 0.06938674\n",
      "Iteration 585, loss = 0.06919250\n",
      "Iteration 586, loss = 0.06889735\n",
      "Iteration 587, loss = 0.06861172\n",
      "Iteration 588, loss = 0.06837923\n",
      "Iteration 589, loss = 0.06811209\n",
      "Iteration 590, loss = 0.06788035\n",
      "Iteration 591, loss = 0.06759362\n",
      "Iteration 592, loss = 0.06742653\n",
      "Iteration 593, loss = 0.06713006\n",
      "Iteration 594, loss = 0.06689216\n",
      "Iteration 595, loss = 0.06665375\n",
      "Iteration 596, loss = 0.06637559\n",
      "Iteration 597, loss = 0.06618296\n",
      "Iteration 598, loss = 0.06591215\n",
      "Iteration 599, loss = 0.06566883\n",
      "Iteration 600, loss = 0.06543205\n",
      "Iteration 601, loss = 0.06514565\n",
      "Iteration 602, loss = 0.06492123\n",
      "Iteration 603, loss = 0.06473855\n",
      "Iteration 604, loss = 0.06448179\n",
      "Iteration 605, loss = 0.06422599\n",
      "Iteration 606, loss = 0.06399801\n",
      "Iteration 607, loss = 0.06376963\n",
      "Iteration 608, loss = 0.06352262\n",
      "Iteration 609, loss = 0.06330551\n",
      "Iteration 610, loss = 0.06312416\n",
      "Iteration 611, loss = 0.06287222\n",
      "Iteration 612, loss = 0.06261819\n",
      "Iteration 613, loss = 0.06239848\n",
      "Iteration 614, loss = 0.06216018\n",
      "Iteration 615, loss = 0.06194699\n",
      "Iteration 616, loss = 0.06177187\n",
      "Iteration 617, loss = 0.06147785\n",
      "Iteration 618, loss = 0.06128706\n",
      "Iteration 619, loss = 0.06104504\n",
      "Iteration 620, loss = 0.06087134\n",
      "Iteration 621, loss = 0.06063742\n",
      "Iteration 622, loss = 0.06041005\n",
      "Iteration 623, loss = 0.06019181\n",
      "Iteration 624, loss = 0.05996185\n",
      "Iteration 625, loss = 0.05977493\n",
      "Iteration 626, loss = 0.05954684\n",
      "Iteration 627, loss = 0.05940627\n",
      "Iteration 628, loss = 0.05913589\n",
      "Iteration 629, loss = 0.05892400\n",
      "Iteration 630, loss = 0.05871117\n",
      "Iteration 631, loss = 0.05851042\n",
      "Iteration 632, loss = 0.05830723\n",
      "Iteration 633, loss = 0.05810219\n",
      "Iteration 634, loss = 0.05788146\n",
      "Iteration 635, loss = 0.05772185\n",
      "Iteration 636, loss = 0.05750632\n",
      "Iteration 637, loss = 0.05729720\n",
      "Iteration 638, loss = 0.05712963\n",
      "Iteration 639, loss = 0.05688878\n",
      "Iteration 640, loss = 0.05668734\n",
      "Iteration 641, loss = 0.05650206\n",
      "Iteration 642, loss = 0.05637800\n",
      "Iteration 643, loss = 0.05612711\n",
      "Iteration 644, loss = 0.05593277\n",
      "Iteration 645, loss = 0.05574062\n",
      "Iteration 646, loss = 0.05556557\n",
      "Iteration 647, loss = 0.05536607\n",
      "Iteration 648, loss = 0.05517109\n",
      "Iteration 649, loss = 0.05497001\n",
      "Iteration 650, loss = 0.05481675\n",
      "Iteration 651, loss = 0.05458073\n",
      "Iteration 652, loss = 0.05438063\n",
      "Iteration 653, loss = 0.05430359\n",
      "Iteration 654, loss = 0.05402008\n",
      "Iteration 655, loss = 0.05387071\n",
      "Iteration 656, loss = 0.05366283\n",
      "Iteration 657, loss = 0.05348261\n",
      "Iteration 658, loss = 0.05335639\n",
      "Iteration 659, loss = 0.05313359\n",
      "Iteration 660, loss = 0.05297366\n",
      "Iteration 661, loss = 0.05279253\n",
      "Iteration 662, loss = 0.05263373\n",
      "Iteration 663, loss = 0.05241285\n",
      "Iteration 664, loss = 0.05223456\n",
      "Iteration 665, loss = 0.05205330\n",
      "Iteration 666, loss = 0.05188346\n",
      "Iteration 667, loss = 0.05170669\n",
      "Iteration 668, loss = 0.05152385\n",
      "Iteration 669, loss = 0.05139024\n",
      "Iteration 670, loss = 0.05118271\n",
      "Iteration 671, loss = 0.05103271\n",
      "Iteration 672, loss = 0.05090423\n",
      "Iteration 673, loss = 0.05068742\n",
      "Iteration 674, loss = 0.05050882\n",
      "Iteration 675, loss = 0.05033177\n",
      "Iteration 676, loss = 0.05017501\n",
      "Iteration 677, loss = 0.05004183\n",
      "Iteration 678, loss = 0.04983822\n",
      "Iteration 679, loss = 0.04968600\n",
      "Iteration 680, loss = 0.04954712\n",
      "Iteration 681, loss = 0.04938828\n",
      "Iteration 682, loss = 0.04921524\n",
      "Iteration 683, loss = 0.04903310\n",
      "Iteration 684, loss = 0.04888231\n",
      "Iteration 685, loss = 0.04873262\n",
      "Iteration 686, loss = 0.04860451\n",
      "Iteration 687, loss = 0.04839227\n",
      "Iteration 688, loss = 0.04828239\n",
      "Iteration 689, loss = 0.04808233\n",
      "Iteration 690, loss = 0.04793742\n",
      "Iteration 691, loss = 0.04778081\n",
      "Iteration 692, loss = 0.04761087\n",
      "Iteration 693, loss = 0.04744661\n",
      "Iteration 694, loss = 0.04730458\n",
      "Iteration 695, loss = 0.04720611\n",
      "Iteration 696, loss = 0.04701117\n",
      "Iteration 697, loss = 0.04688694\n",
      "Iteration 698, loss = 0.04670287\n",
      "Iteration 699, loss = 0.04654794\n",
      "Iteration 700, loss = 0.04640426\n",
      "Iteration 701, loss = 0.04624310\n",
      "Iteration 702, loss = 0.04614495\n",
      "Iteration 703, loss = 0.04597153\n",
      "Iteration 704, loss = 0.04579613\n",
      "Iteration 705, loss = 0.04569601\n",
      "Iteration 706, loss = 0.04553316\n",
      "Iteration 707, loss = 0.04536799\n",
      "Iteration 708, loss = 0.04524973\n",
      "Iteration 709, loss = 0.04512641\n",
      "Iteration 710, loss = 0.04497597\n",
      "Iteration 711, loss = 0.04479089\n",
      "Iteration 712, loss = 0.04468257\n",
      "Iteration 713, loss = 0.04458093\n",
      "Iteration 714, loss = 0.04444170\n",
      "Iteration 715, loss = 0.04422142\n",
      "Iteration 716, loss = 0.04407953\n",
      "Iteration 717, loss = 0.04395181\n",
      "Iteration 718, loss = 0.04381320\n",
      "Iteration 719, loss = 0.04370419\n",
      "Iteration 720, loss = 0.04352031\n",
      "Iteration 721, loss = 0.04341704\n",
      "Iteration 722, loss = 0.04326902\n",
      "Iteration 723, loss = 0.04312570\n",
      "Iteration 724, loss = 0.04301093\n",
      "Iteration 725, loss = 0.04287916\n",
      "Iteration 726, loss = 0.04270557\n",
      "Iteration 727, loss = 0.04257487\n",
      "Iteration 728, loss = 0.04243614\n",
      "Iteration 729, loss = 0.04233119\n",
      "Iteration 730, loss = 0.04220032\n",
      "Iteration 731, loss = 0.04204188\n",
      "Iteration 732, loss = 0.04192235\n",
      "Iteration 733, loss = 0.04180597\n",
      "Iteration 734, loss = 0.04165756\n",
      "Iteration 735, loss = 0.04153438\n",
      "Iteration 736, loss = 0.04141876\n",
      "Iteration 737, loss = 0.04125658\n",
      "Iteration 738, loss = 0.04116048\n",
      "Iteration 739, loss = 0.04107430\n",
      "Iteration 740, loss = 0.04090124\n",
      "Iteration 741, loss = 0.04076208\n",
      "Iteration 742, loss = 0.04067443\n",
      "Iteration 743, loss = 0.04050820\n",
      "Iteration 744, loss = 0.04039547\n",
      "Iteration 745, loss = 0.04030267\n",
      "Iteration 746, loss = 0.04014729\n",
      "Iteration 747, loss = 0.04004584\n",
      "Iteration 748, loss = 0.03992943\n",
      "Iteration 749, loss = 0.03976859\n",
      "Iteration 750, loss = 0.03962806\n",
      "Iteration 751, loss = 0.03958953\n",
      "Iteration 752, loss = 0.03941410\n",
      "Iteration 753, loss = 0.03932722\n",
      "Iteration 754, loss = 0.03921804\n",
      "Iteration 755, loss = 0.03909554\n",
      "Iteration 756, loss = 0.03898202\n",
      "Iteration 757, loss = 0.03884009\n",
      "Iteration 758, loss = 0.03870585\n",
      "Iteration 759, loss = 0.03859630\n",
      "Iteration 760, loss = 0.03847521\n",
      "Iteration 761, loss = 0.03837290\n",
      "Iteration 762, loss = 0.03825171\n",
      "Iteration 763, loss = 0.03811589\n",
      "Iteration 764, loss = 0.03804696\n",
      "Iteration 765, loss = 0.03790106\n",
      "Iteration 766, loss = 0.03777918\n",
      "Iteration 767, loss = 0.03773680\n",
      "Iteration 768, loss = 0.03757341\n",
      "Iteration 769, loss = 0.03743585\n",
      "Iteration 770, loss = 0.03732311\n",
      "Iteration 771, loss = 0.03726854\n",
      "Iteration 772, loss = 0.03711232\n",
      "Iteration 773, loss = 0.03702116\n",
      "Iteration 774, loss = 0.03690225\n",
      "Iteration 775, loss = 0.03678883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 776, loss = 0.03668449\n",
      "Iteration 777, loss = 0.03659683\n",
      "Iteration 778, loss = 0.03646854\n",
      "Iteration 779, loss = 0.03633543\n",
      "Iteration 780, loss = 0.03628916\n",
      "Iteration 781, loss = 0.03613615\n",
      "Iteration 782, loss = 0.03605955\n",
      "Iteration 783, loss = 0.03597159\n",
      "Iteration 784, loss = 0.03585831\n",
      "Iteration 785, loss = 0.03572424\n",
      "Iteration 786, loss = 0.03558690\n",
      "Iteration 787, loss = 0.03549464\n",
      "Iteration 788, loss = 0.03544085\n",
      "Iteration 789, loss = 0.03533992\n",
      "Iteration 790, loss = 0.03519668\n",
      "Iteration 791, loss = 0.03510638\n",
      "Iteration 792, loss = 0.03499177\n",
      "Iteration 793, loss = 0.03489394\n",
      "Iteration 794, loss = 0.03478551\n",
      "Iteration 795, loss = 0.03468701\n",
      "Iteration 796, loss = 0.03455760\n",
      "Iteration 797, loss = 0.03450200\n",
      "Iteration 798, loss = 0.03446074\n",
      "Iteration 799, loss = 0.03436210\n",
      "Iteration 800, loss = 0.03415429\n",
      "Iteration 801, loss = 0.03408715\n",
      "Iteration 802, loss = 0.03401722\n",
      "Iteration 803, loss = 0.03389243\n",
      "Iteration 804, loss = 0.03377079\n",
      "Iteration 805, loss = 0.03367182\n",
      "Iteration 806, loss = 0.03363445\n",
      "Iteration 807, loss = 0.03349807\n",
      "Iteration 808, loss = 0.03337506\n",
      "Iteration 809, loss = 0.03329520\n",
      "Iteration 810, loss = 0.03320140\n",
      "Iteration 811, loss = 0.03309952\n",
      "Iteration 812, loss = 0.03305461\n",
      "Iteration 813, loss = 0.03293778\n",
      "Iteration 814, loss = 0.03290091\n",
      "Iteration 815, loss = 0.03275636\n",
      "Iteration 816, loss = 0.03268557\n",
      "Iteration 817, loss = 0.03255552\n",
      "Iteration 818, loss = 0.03246078\n",
      "Iteration 819, loss = 0.03244419\n",
      "Iteration 820, loss = 0.03228179\n",
      "Iteration 821, loss = 0.03225244\n",
      "Iteration 822, loss = 0.03211158\n",
      "Iteration 823, loss = 0.03197657\n",
      "Iteration 824, loss = 0.03187124\n",
      "Iteration 825, loss = 0.03179879\n",
      "Iteration 826, loss = 0.03170125\n",
      "Iteration 827, loss = 0.03174233\n",
      "Iteration 828, loss = 0.03156127\n",
      "Iteration 829, loss = 0.03143449\n",
      "Iteration 830, loss = 0.03135348\n",
      "Iteration 831, loss = 0.03130775\n",
      "Iteration 832, loss = 0.03121906\n",
      "Iteration 833, loss = 0.03109325\n",
      "Iteration 834, loss = 0.03102359\n",
      "Iteration 835, loss = 0.03095515\n",
      "Iteration 836, loss = 0.03086481\n",
      "Iteration 837, loss = 0.03074702\n",
      "Iteration 838, loss = 0.03068674\n",
      "Iteration 839, loss = 0.03059904\n",
      "Iteration 840, loss = 0.03055610\n",
      "Iteration 841, loss = 0.03045254\n",
      "Iteration 842, loss = 0.03034815\n",
      "Iteration 843, loss = 0.03022473\n",
      "Iteration 844, loss = 0.03013248\n",
      "Iteration 845, loss = 0.03018329\n",
      "Iteration 846, loss = 0.03003537\n",
      "Iteration 847, loss = 0.02989604\n",
      "Iteration 848, loss = 0.02981769\n",
      "Iteration 849, loss = 0.02975405\n",
      "Iteration 850, loss = 0.02965330\n",
      "Iteration 851, loss = 0.02954355\n",
      "Iteration 852, loss = 0.02949375\n",
      "Iteration 853, loss = 0.02943883\n",
      "Iteration 854, loss = 0.02937276\n",
      "Iteration 855, loss = 0.02927224\n",
      "Iteration 856, loss = 0.02920788\n",
      "Iteration 857, loss = 0.02911310\n",
      "Iteration 858, loss = 0.02899863\n",
      "Iteration 859, loss = 0.02891138\n",
      "Iteration 860, loss = 0.02884355\n",
      "Iteration 861, loss = 0.02880816\n",
      "Iteration 862, loss = 0.02871007\n",
      "Iteration 863, loss = 0.02862428\n",
      "Iteration 864, loss = 0.02857996\n",
      "Iteration 865, loss = 0.02847697\n",
      "Iteration 866, loss = 0.02836241\n",
      "Iteration 867, loss = 0.02836099\n",
      "Iteration 868, loss = 0.02824560\n",
      "Iteration 869, loss = 0.02817040\n",
      "Iteration 870, loss = 0.02820110\n",
      "Iteration 871, loss = 0.02799361\n",
      "Iteration 872, loss = 0.02793685\n",
      "Iteration 873, loss = 0.02787441\n",
      "Iteration 874, loss = 0.02776163\n",
      "Iteration 875, loss = 0.02770412\n",
      "Iteration 876, loss = 0.02766004\n",
      "Iteration 877, loss = 0.02755763\n",
      "Iteration 878, loss = 0.02748298\n",
      "Iteration 879, loss = 0.02745489\n",
      "Iteration 880, loss = 0.02731704\n",
      "Iteration 881, loss = 0.02725567\n",
      "Iteration 882, loss = 0.02722380\n",
      "Iteration 883, loss = 0.02714339\n",
      "Iteration 884, loss = 0.02704216\n",
      "Iteration 885, loss = 0.02693947\n",
      "Iteration 886, loss = 0.02689312\n",
      "Iteration 887, loss = 0.02686284\n",
      "Iteration 888, loss = 0.02674003\n",
      "Iteration 889, loss = 0.02663555\n",
      "Iteration 890, loss = 0.02657515\n",
      "Iteration 891, loss = 0.02654078\n",
      "Iteration 892, loss = 0.02646943\n",
      "Iteration 893, loss = 0.02636624\n",
      "Iteration 894, loss = 0.02631018\n",
      "Iteration 895, loss = 0.02626485\n",
      "Iteration 896, loss = 0.02621967\n",
      "Iteration 897, loss = 0.02614244\n",
      "Iteration 898, loss = 0.02604202\n",
      "Iteration 899, loss = 0.02597115\n",
      "Iteration 900, loss = 0.02592405\n",
      "Iteration 901, loss = 0.02581270\n",
      "Iteration 902, loss = 0.02577058\n",
      "Iteration 903, loss = 0.02581248\n",
      "Iteration 904, loss = 0.02568636\n",
      "Iteration 905, loss = 0.02555893\n",
      "Iteration 906, loss = 0.02547603\n",
      "Iteration 907, loss = 0.02543090\n",
      "Iteration 908, loss = 0.02534505\n",
      "Iteration 909, loss = 0.02530521\n",
      "Iteration 910, loss = 0.02520730\n",
      "Iteration 911, loss = 0.02514189\n",
      "Iteration 912, loss = 0.02507208\n",
      "Iteration 913, loss = 0.02503279\n",
      "Iteration 914, loss = 0.02496555\n",
      "Iteration 915, loss = 0.02490441\n",
      "Iteration 916, loss = 0.02481562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, verbose=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_credit = MLPClassifier(max_iter=1500, verbose=True,tol=0.000100,\n",
    "                             solver='adam', activation='relu',\n",
    "                             hidden_layer_sizes=(2,2))\n",
    "neural_credit.fit(x_credit_train, y_credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7d9fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:32:04.499577Z",
     "start_time": "2021-09-25T18:32:04.449567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = neural_credit.predict(x_credit_test)\n",
    "accuracy_score(y_credit_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2614afed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:32:36.031236Z",
     "start_time": "2021-09-25T18:32:35.873689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619b9658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:33:00.561016Z",
     "start_time": "2021-09-25T18:33:00.292956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_credit_test, pred, figsize=(7,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f9558",
   "metadata": {},
   "source": [
    "## Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e4f4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:42:27.514738Z",
     "start_time": "2021-09-25T18:42:27.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/census.pkl', 'rb') as f:\n",
    "    x_census_train, y_census_train, x_census_test, y_census_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cf971",
   "metadata": {},
   "source": [
    "Entradas + Sa√≠das / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2ec60b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:54:41.111022Z",
     "start_time": "2021-09-25T18:54:41.098937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Camadas ocultas\n",
    "(108 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447bae2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:54:37.901576Z",
     "start_time": "2021-09-25T18:53:10.488675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38211383\n",
      "Iteration 2, loss = 0.32592872\n",
      "Iteration 3, loss = 0.31388404\n",
      "Iteration 4, loss = 0.30681003\n",
      "Iteration 5, loss = 0.30155820\n",
      "Iteration 6, loss = 0.29745448\n",
      "Iteration 7, loss = 0.29446514\n",
      "Iteration 8, loss = 0.29147463\n",
      "Iteration 9, loss = 0.28938213\n",
      "Iteration 10, loss = 0.28685879\n",
      "Iteration 11, loss = 0.28489189\n",
      "Iteration 12, loss = 0.28333170\n",
      "Iteration 13, loss = 0.28104656\n",
      "Iteration 14, loss = 0.27896721\n",
      "Iteration 15, loss = 0.27890443\n",
      "Iteration 16, loss = 0.27555822\n",
      "Iteration 17, loss = 0.27477122\n",
      "Iteration 18, loss = 0.27271842\n",
      "Iteration 19, loss = 0.27087754\n",
      "Iteration 20, loss = 0.26988392\n",
      "Iteration 21, loss = 0.26811439\n",
      "Iteration 22, loss = 0.26642973\n",
      "Iteration 23, loss = 0.26477918\n",
      "Iteration 24, loss = 0.26346735\n",
      "Iteration 25, loss = 0.26241051\n",
      "Iteration 26, loss = 0.26035553\n",
      "Iteration 27, loss = 0.25885180\n",
      "Iteration 28, loss = 0.25763170\n",
      "Iteration 29, loss = 0.25624135\n",
      "Iteration 30, loss = 0.25452253\n",
      "Iteration 31, loss = 0.25356997\n",
      "Iteration 32, loss = 0.25271700\n",
      "Iteration 33, loss = 0.25034262\n",
      "Iteration 34, loss = 0.24923566\n",
      "Iteration 35, loss = 0.24953482\n",
      "Iteration 36, loss = 0.24712149\n",
      "Iteration 37, loss = 0.24561093\n",
      "Iteration 38, loss = 0.24486850\n",
      "Iteration 39, loss = 0.24430145\n",
      "Iteration 40, loss = 0.24259745\n",
      "Iteration 41, loss = 0.24136622\n",
      "Iteration 42, loss = 0.24080038\n",
      "Iteration 43, loss = 0.23920688\n",
      "Iteration 44, loss = 0.23795506\n",
      "Iteration 45, loss = 0.23787661\n",
      "Iteration 46, loss = 0.23617467\n",
      "Iteration 47, loss = 0.23498712\n",
      "Iteration 48, loss = 0.23339821\n",
      "Iteration 49, loss = 0.23268309\n",
      "Iteration 50, loss = 0.23228050\n",
      "Iteration 51, loss = 0.23092294\n",
      "Iteration 52, loss = 0.22993630\n",
      "Iteration 53, loss = 0.22987439\n",
      "Iteration 54, loss = 0.22838929\n",
      "Iteration 55, loss = 0.22682940\n",
      "Iteration 56, loss = 0.22605985\n",
      "Iteration 57, loss = 0.22489281\n",
      "Iteration 58, loss = 0.22514042\n",
      "Iteration 59, loss = 0.22423521\n",
      "Iteration 60, loss = 0.22264289\n",
      "Iteration 61, loss = 0.22341472\n",
      "Iteration 62, loss = 0.22191854\n",
      "Iteration 63, loss = 0.22158146\n",
      "Iteration 64, loss = 0.22141323\n",
      "Iteration 65, loss = 0.21906047\n",
      "Iteration 66, loss = 0.21882756\n",
      "Iteration 67, loss = 0.21860935\n",
      "Iteration 68, loss = 0.21781037\n",
      "Iteration 69, loss = 0.21747737\n",
      "Iteration 70, loss = 0.21627931\n",
      "Iteration 71, loss = 0.21582349\n",
      "Iteration 72, loss = 0.21440354\n",
      "Iteration 73, loss = 0.21328713\n",
      "Iteration 74, loss = 0.21250461\n",
      "Iteration 75, loss = 0.21231611\n",
      "Iteration 76, loss = 0.21173195\n",
      "Iteration 77, loss = 0.20988146\n",
      "Iteration 78, loss = 0.21061619\n",
      "Iteration 79, loss = 0.21021441\n",
      "Iteration 80, loss = 0.21115304\n",
      "Iteration 81, loss = 0.20886073\n",
      "Iteration 82, loss = 0.20990173\n",
      "Iteration 83, loss = 0.20801991\n",
      "Iteration 84, loss = 0.20646992\n",
      "Iteration 85, loss = 0.20594524\n",
      "Iteration 86, loss = 0.20698454\n",
      "Iteration 87, loss = 0.20542731\n",
      "Iteration 88, loss = 0.20422427\n",
      "Iteration 89, loss = 0.20452834\n",
      "Iteration 90, loss = 0.20342281\n",
      "Iteration 91, loss = 0.20507810\n",
      "Iteration 92, loss = 0.20219657\n",
      "Iteration 93, loss = 0.20194808\n",
      "Iteration 94, loss = 0.20100006\n",
      "Iteration 95, loss = 0.20346842\n",
      "Iteration 96, loss = 0.20097166\n",
      "Iteration 97, loss = 0.20041708\n",
      "Iteration 98, loss = 0.19932616\n",
      "Iteration 99, loss = 0.19949120\n",
      "Iteration 100, loss = 0.19861910\n",
      "Iteration 101, loss = 0.19881294\n",
      "Iteration 102, loss = 0.19797235\n",
      "Iteration 103, loss = 0.19788011\n",
      "Iteration 104, loss = 0.19702930\n",
      "Iteration 105, loss = 0.19725861\n",
      "Iteration 106, loss = 0.19698501\n",
      "Iteration 107, loss = 0.19560307\n",
      "Iteration 108, loss = 0.19489658\n",
      "Iteration 109, loss = 0.19498839\n",
      "Iteration 110, loss = 0.19354085\n",
      "Iteration 111, loss = 0.19418673\n",
      "Iteration 112, loss = 0.19413941\n",
      "Iteration 113, loss = 0.19361082\n",
      "Iteration 114, loss = 0.19212592\n",
      "Iteration 115, loss = 0.19195544\n",
      "Iteration 116, loss = 0.19298595\n",
      "Iteration 117, loss = 0.19101793\n",
      "Iteration 118, loss = 0.19144364\n",
      "Iteration 119, loss = 0.18958486\n",
      "Iteration 120, loss = 0.18981077\n",
      "Iteration 121, loss = 0.18971239\n",
      "Iteration 122, loss = 0.19007780\n",
      "Iteration 123, loss = 0.18852814\n",
      "Iteration 124, loss = 0.18824776\n",
      "Iteration 125, loss = 0.18916313\n",
      "Iteration 126, loss = 0.18830589\n",
      "Iteration 127, loss = 0.18860590\n",
      "Iteration 128, loss = 0.18780377\n",
      "Iteration 129, loss = 0.18673405\n",
      "Iteration 130, loss = 0.18685011\n",
      "Iteration 131, loss = 0.18607946\n",
      "Iteration 132, loss = 0.18680288\n",
      "Iteration 133, loss = 0.18626241\n",
      "Iteration 134, loss = 0.18556171\n",
      "Iteration 135, loss = 0.18506348\n",
      "Iteration 136, loss = 0.18446056\n",
      "Iteration 137, loss = 0.18424724\n",
      "Iteration 138, loss = 0.18332861\n",
      "Iteration 139, loss = 0.18276479\n",
      "Iteration 140, loss = 0.18357782\n",
      "Iteration 141, loss = 0.18365151\n",
      "Iteration 142, loss = 0.18449176\n",
      "Iteration 143, loss = 0.18113186\n",
      "Iteration 144, loss = 0.18253716\n",
      "Iteration 145, loss = 0.18208282\n",
      "Iteration 146, loss = 0.18266969\n",
      "Iteration 147, loss = 0.18128201\n",
      "Iteration 148, loss = 0.18138702\n",
      "Iteration 149, loss = 0.18004994\n",
      "Iteration 150, loss = 0.18021626\n",
      "Iteration 151, loss = 0.17950588\n",
      "Iteration 152, loss = 0.17841830\n",
      "Iteration 153, loss = 0.17936115\n",
      "Iteration 154, loss = 0.17845890\n",
      "Iteration 155, loss = 0.17866801\n",
      "Iteration 156, loss = 0.17866483\n",
      "Iteration 157, loss = 0.17809574\n",
      "Iteration 158, loss = 0.17889712\n",
      "Iteration 159, loss = 0.17861400\n",
      "Iteration 160, loss = 0.17723754\n",
      "Iteration 161, loss = 0.17676575\n",
      "Iteration 162, loss = 0.17706816\n",
      "Iteration 163, loss = 0.17740206\n",
      "Iteration 164, loss = 0.17564735\n",
      "Iteration 165, loss = 0.17544490\n",
      "Iteration 166, loss = 0.17517013\n",
      "Iteration 167, loss = 0.17578672\n",
      "Iteration 168, loss = 0.17324540\n",
      "Iteration 169, loss = 0.17426598\n",
      "Iteration 170, loss = 0.17413531\n",
      "Iteration 171, loss = 0.17337590\n",
      "Iteration 172, loss = 0.17413869\n",
      "Iteration 173, loss = 0.17401754\n",
      "Iteration 174, loss = 0.17371822\n",
      "Iteration 175, loss = 0.17304835\n",
      "Iteration 176, loss = 0.17282882\n",
      "Iteration 177, loss = 0.17271398\n",
      "Iteration 178, loss = 0.17180243\n",
      "Iteration 179, loss = 0.17289817\n",
      "Iteration 180, loss = 0.17203480\n",
      "Iteration 181, loss = 0.17326994\n",
      "Iteration 182, loss = 0.17107614\n",
      "Iteration 183, loss = 0.17026244\n",
      "Iteration 184, loss = 0.17072372\n",
      "Iteration 185, loss = 0.17176952\n",
      "Iteration 186, loss = 0.16966705\n",
      "Iteration 187, loss = 0.17080327\n",
      "Iteration 188, loss = 0.17071547\n",
      "Iteration 189, loss = 0.16908177\n",
      "Iteration 190, loss = 0.16958474\n",
      "Iteration 191, loss = 0.16878435\n",
      "Iteration 192, loss = 0.16852209\n",
      "Iteration 193, loss = 0.16844255\n",
      "Iteration 194, loss = 0.16766736\n",
      "Iteration 195, loss = 0.16733486\n",
      "Iteration 196, loss = 0.16805635\n",
      "Iteration 197, loss = 0.16814004\n",
      "Iteration 198, loss = 0.16878020\n",
      "Iteration 199, loss = 0.16641503\n",
      "Iteration 200, loss = 0.16574419\n",
      "Iteration 201, loss = 0.16665807\n",
      "Iteration 202, loss = 0.16827897\n",
      "Iteration 203, loss = 0.16630735\n",
      "Iteration 204, loss = 0.16687776\n",
      "Iteration 205, loss = 0.16656591\n",
      "Iteration 206, loss = 0.16434377\n",
      "Iteration 207, loss = 0.16776648\n",
      "Iteration 208, loss = 0.16683839\n",
      "Iteration 209, loss = 0.16486033\n",
      "Iteration 210, loss = 0.16519456\n",
      "Iteration 211, loss = 0.16562674\n",
      "Iteration 212, loss = 0.16581744\n",
      "Iteration 213, loss = 0.16386132\n",
      "Iteration 214, loss = 0.16489916\n",
      "Iteration 215, loss = 0.16394884\n",
      "Iteration 216, loss = 0.16382425\n",
      "Iteration 217, loss = 0.16477141\n",
      "Iteration 218, loss = 0.16268491\n",
      "Iteration 219, loss = 0.16357999\n",
      "Iteration 220, loss = 0.16329028\n",
      "Iteration 221, loss = 0.16177807\n",
      "Iteration 222, loss = 0.16248913\n",
      "Iteration 223, loss = 0.16190193\n",
      "Iteration 224, loss = 0.16263133\n",
      "Iteration 225, loss = 0.16050729\n",
      "Iteration 226, loss = 0.16133280\n",
      "Iteration 227, loss = 0.16292860\n",
      "Iteration 228, loss = 0.16163765\n",
      "Iteration 229, loss = 0.16111545\n",
      "Iteration 230, loss = 0.16084055\n",
      "Iteration 231, loss = 0.16190496\n",
      "Iteration 232, loss = 0.16171913\n",
      "Iteration 233, loss = 0.16061235\n",
      "Iteration 234, loss = 0.15922208\n",
      "Iteration 235, loss = 0.16040353\n",
      "Iteration 236, loss = 0.15945026\n",
      "Iteration 237, loss = 0.16018191\n",
      "Iteration 238, loss = 0.16008802\n",
      "Iteration 239, loss = 0.15890070\n",
      "Iteration 240, loss = 0.15978022\n",
      "Iteration 241, loss = 0.15883714\n",
      "Iteration 242, loss = 0.15840953\n",
      "Iteration 243, loss = 0.16009349\n",
      "Iteration 244, loss = 0.15993486\n",
      "Iteration 245, loss = 0.15805135\n",
      "Iteration 246, loss = 0.16090983\n",
      "Iteration 247, loss = 0.15769345\n",
      "Iteration 248, loss = 0.15843687\n",
      "Iteration 249, loss = 0.15625023\n",
      "Iteration 250, loss = 0.15849923\n",
      "Iteration 251, loss = 0.15871229\n",
      "Iteration 252, loss = 0.15758202\n",
      "Iteration 253, loss = 0.15798447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.15684503\n",
      "Iteration 255, loss = 0.15642559\n",
      "Iteration 256, loss = 0.15786141\n",
      "Iteration 257, loss = 0.15741340\n",
      "Iteration 258, loss = 0.15646436\n",
      "Iteration 259, loss = 0.15627471\n",
      "Iteration 260, loss = 0.15448992\n",
      "Iteration 261, loss = 0.15501438\n",
      "Iteration 262, loss = 0.15636203\n",
      "Iteration 263, loss = 0.15591089\n",
      "Iteration 264, loss = 0.15533154\n",
      "Iteration 265, loss = 0.15675787\n",
      "Iteration 266, loss = 0.15592258\n",
      "Iteration 267, loss = 0.15576478\n",
      "Iteration 268, loss = 0.15347236\n",
      "Iteration 269, loss = 0.15538315\n",
      "Iteration 270, loss = 0.15451623\n",
      "Iteration 271, loss = 0.15438560\n",
      "Iteration 272, loss = 0.15480746\n",
      "Iteration 273, loss = 0.15460385\n",
      "Iteration 274, loss = 0.15344044\n",
      "Iteration 275, loss = 0.15316938\n",
      "Iteration 276, loss = 0.15228978\n",
      "Iteration 277, loss = 0.15363350\n",
      "Iteration 278, loss = 0.15388439\n",
      "Iteration 279, loss = 0.15221360\n",
      "Iteration 280, loss = 0.15192822\n",
      "Iteration 281, loss = 0.15382278\n",
      "Iteration 282, loss = 0.15284929\n",
      "Iteration 283, loss = 0.15204091\n",
      "Iteration 284, loss = 0.15167240\n",
      "Iteration 285, loss = 0.15160491\n",
      "Iteration 286, loss = 0.15190375\n",
      "Iteration 287, loss = 0.15211692\n",
      "Iteration 288, loss = 0.15293002\n",
      "Iteration 289, loss = 0.15174148\n",
      "Iteration 290, loss = 0.15152130\n",
      "Iteration 291, loss = 0.15204802\n",
      "Iteration 292, loss = 0.15132719\n",
      "Iteration 293, loss = 0.15215260\n",
      "Iteration 294, loss = 0.15217180\n",
      "Iteration 295, loss = 0.15090241\n",
      "Iteration 296, loss = 0.15050338\n",
      "Iteration 297, loss = 0.15069677\n",
      "Iteration 298, loss = 0.15257936\n",
      "Iteration 299, loss = 0.15281757\n",
      "Iteration 300, loss = 0.15035852\n",
      "Iteration 301, loss = 0.15046263\n",
      "Iteration 302, loss = 0.14955377\n",
      "Iteration 303, loss = 0.14941918\n",
      "Iteration 304, loss = 0.14864872\n",
      "Iteration 305, loss = 0.14933030\n",
      "Iteration 306, loss = 0.15080320\n",
      "Iteration 307, loss = 0.15029516\n",
      "Iteration 308, loss = 0.14945181\n",
      "Iteration 309, loss = 0.15031822\n",
      "Iteration 310, loss = 0.14902647\n",
      "Iteration 311, loss = 0.14841393\n",
      "Iteration 312, loss = 0.15036976\n",
      "Iteration 313, loss = 0.14767785\n",
      "Iteration 314, loss = 0.14768533\n",
      "Iteration 315, loss = 0.14944741\n",
      "Iteration 316, loss = 0.14677558\n",
      "Iteration 317, loss = 0.14971775\n",
      "Iteration 318, loss = 0.14757277\n",
      "Iteration 319, loss = 0.14647253\n",
      "Iteration 320, loss = 0.14784799\n",
      "Iteration 321, loss = 0.14816683\n",
      "Iteration 322, loss = 0.14735947\n",
      "Iteration 323, loss = 0.14660653\n",
      "Iteration 324, loss = 0.14801022\n",
      "Iteration 325, loss = 0.14641275\n",
      "Iteration 326, loss = 0.14611067\n",
      "Iteration 327, loss = 0.14649644\n",
      "Iteration 328, loss = 0.14569646\n",
      "Iteration 329, loss = 0.14599324\n",
      "Iteration 330, loss = 0.14673621\n",
      "Iteration 331, loss = 0.14764182\n",
      "Iteration 332, loss = 0.14973625\n",
      "Iteration 333, loss = 0.14548470\n",
      "Iteration 334, loss = 0.14438112\n",
      "Iteration 335, loss = 0.14683401\n",
      "Iteration 336, loss = 0.14496808\n",
      "Iteration 337, loss = 0.14664558\n",
      "Iteration 338, loss = 0.14466598\n",
      "Iteration 339, loss = 0.14585324\n",
      "Iteration 340, loss = 0.14496597\n",
      "Iteration 341, loss = 0.14634077\n",
      "Iteration 342, loss = 0.14403329\n",
      "Iteration 343, loss = 0.14444430\n",
      "Iteration 344, loss = 0.14553941\n",
      "Iteration 345, loss = 0.14498648\n",
      "Iteration 346, loss = 0.14424661\n",
      "Iteration 347, loss = 0.14382973\n",
      "Iteration 348, loss = 0.14419441\n",
      "Iteration 349, loss = 0.14460520\n",
      "Iteration 350, loss = 0.14483692\n",
      "Iteration 351, loss = 0.14329669\n",
      "Iteration 352, loss = 0.14295830\n",
      "Iteration 353, loss = 0.14466742\n",
      "Iteration 354, loss = 0.14305047\n",
      "Iteration 355, loss = 0.14446168\n",
      "Iteration 356, loss = 0.14277151\n",
      "Iteration 357, loss = 0.14237387\n",
      "Iteration 358, loss = 0.14250961\n",
      "Iteration 359, loss = 0.14255799\n",
      "Iteration 360, loss = 0.14396833\n",
      "Iteration 361, loss = 0.14311640\n",
      "Iteration 362, loss = 0.14416111\n",
      "Iteration 363, loss = 0.14324807\n",
      "Iteration 364, loss = 0.14122962\n",
      "Iteration 365, loss = 0.14249561\n",
      "Iteration 366, loss = 0.14190611\n",
      "Iteration 367, loss = 0.14325386\n",
      "Iteration 368, loss = 0.14113030\n",
      "Iteration 369, loss = 0.14233147\n",
      "Iteration 370, loss = 0.14380384\n",
      "Iteration 371, loss = 0.14282551\n",
      "Iteration 372, loss = 0.14252490\n",
      "Iteration 373, loss = 0.14161305\n",
      "Iteration 374, loss = 0.14358203\n",
      "Iteration 375, loss = 0.14000676\n",
      "Iteration 376, loss = 0.14064488\n",
      "Iteration 377, loss = 0.14121360\n",
      "Iteration 378, loss = 0.14004934\n",
      "Iteration 379, loss = 0.13973213\n",
      "Iteration 380, loss = 0.14075615\n",
      "Iteration 381, loss = 0.14329424\n",
      "Iteration 382, loss = 0.14100279\n",
      "Iteration 383, loss = 0.14146985\n",
      "Iteration 384, loss = 0.14158531\n",
      "Iteration 385, loss = 0.13982540\n",
      "Iteration 386, loss = 0.14103775\n",
      "Iteration 387, loss = 0.13854385\n",
      "Iteration 388, loss = 0.14161173\n",
      "Iteration 389, loss = 0.13815875\n",
      "Iteration 390, loss = 0.13977686\n",
      "Iteration 391, loss = 0.13987469\n",
      "Iteration 392, loss = 0.13979313\n",
      "Iteration 393, loss = 0.13948856\n",
      "Iteration 394, loss = 0.13925314\n",
      "Iteration 395, loss = 0.13939791\n",
      "Iteration 396, loss = 0.13962114\n",
      "Iteration 397, loss = 0.13985779\n",
      "Iteration 398, loss = 0.14048667\n",
      "Iteration 399, loss = 0.13911174\n",
      "Iteration 400, loss = 0.13948767\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_census = MLPClassifier(max_iter=1000, verbose=True, tol=0.000010,\n",
    "                             hidden_layer_sizes=(55,55))\n",
    "neural_census.fit(x_census_train, y_census_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c296672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:55:16.718309Z",
     "start_time": "2021-09-25T18:55:16.679301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8169907881269192"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = neural_census.predict(x_census_test)\n",
    "accuracy_score(y_census_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa53098e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:55:30.364909Z",
     "start_time": "2021-09-25T18:55:30.248883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.87      0.88      3693\n",
      "        >50K       0.62      0.65      0.63      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.75      0.76      0.76      4885\n",
      "weighted avg       0.82      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_test, pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe455c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T18:56:00.894265Z",
     "start_time": "2021-09-25T18:56:00.677217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_census_test, pred1, figsize=(7,7));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
